
@article{sailynoja_graphical_2022,
	title = {Graphical test for discrete uniformity and its applications in goodness-of-fit evaluation and multiple sample comparison},
	volume = {32},
	issn = {1573-1375},
	url = {https://doi.org/10.1007/s11222-022-10090-6},
	doi = {10.1007/s11222-022-10090-6},
	abstract = {Assessing goodness of fit to a given distribution plays an important role in computational statistics. The probability integral transformation (PIT) can be used to convert the question of whether a given sample originates from a reference distribution into a problem of testing for uniformity. We present new simulation- and optimization-based methods to obtain simultaneous confidence bands for the whole empirical cumulative distribution function (ECDF) of the PIT values under the assumption of uniformity. Simultaneous confidence bands correspond to such confidence intervals at each point that jointly satisfy a desired coverage. These methods can also be applied in cases where the reference distribution is represented only by a finite sample, which is useful, for example, for simulation-based calibration. The confidence bands provide an intuitive ECDF-based graphical test for uniformity, which also provides useful information on the quality of the discrepancy. We further extend the simulation and optimization methods to determine simultaneous confidence bands for testing whether multiple samples come from the same underlying distribution. This multiple sample comparison test is useful, for example, as a complementary diagnostic in multi-chain Markov chain Monte Carlo (MCMC) convergence diagnostics, where most currently used convergence diagnostics provide a single diagnostic value, but do not usually offer insight into the nature of the deviation. We provide numerical experiments to assess the properties of the tests using both simulated and real-world data and give recommendations on their practical application in computational statistics workflows.},
	language = {en},
	number = {2},
	urldate = {2023-06-08},
	journal = {Statistics and Computing},
	author = {Säilynoja, Teemu and Bürkner, Paul-Christian and Vehtari, Aki},
	month = mar,
	year = {2022},
	keywords = {ECDF, MCMC convergence diagnostic, PIT, Simulation-based calibration, Uniformity test},
	pages = {32},
}

@article{kumar_arviz_2019,
	title = {{ArviZ} a unified library for exploratory analysis of {Bayesian} models in {Python}},
	volume = {4},
	issn = {2475-9066},
	url = {http://joss.theoj.org/papers/10.21105/joss.01143},
	doi = {10.21105/joss.01143},
	number = {33},
	urldate = {2023-04-18},
	journal = {Journal of Open Source Software},
	author = {Kumar, Ravin and Carroll, Colin and Hartikainen, Ari and Martin, Osvaldo},
	month = jan,
	year = {2019},
	pages = {1143},
}

@book{wilke_fundamentals_2019,
	title = {Fundamentals of {Data} {Visualization}: {A} {Primer} on {Making} {Informative} and {Compelling} {Figures}},
	isbn = {9781492031055},
	shorttitle = {Fundamentals of {Data} {Visualization}},
	url = {https://clauswilke.com/dataviz/},
	abstract = {Effective visualization is the best way to communicate information from the increasingly large and complex datasets in the natural and social sciences. But with the increasing power of visualization software today, scientists, engineers, and business analysts often have to navigate a bewildering array of visualization choices and options.This practical book takes you through many commonly encountered visualization problems, and it provides guidelines on how to turn large datasets into clear and compelling figures. What visualization type is best for the story you want to tell? How do you make informative figures that are visually pleasing? Author Claus O. Wilke teaches you the elements most critical to successful data visualization.Explore the basic concepts of color as a tool to highlight, distinguish, or represent a valueUnderstand the importance of redundant coding to ensure you provide key information in multiple waysUse the book’s visualizations directory, a graphical guide to commonly used types of data visualizationsGet extensive examples of good and bad figuresLearn how to use figures in a document or report and how employ them effectively to tell a compelling story},
	language = {en},
	publisher = {O'Reilly Media, Inc.},
	author = {Wilke, Claus O.},
	month = mar,
	year = {2019},
	keywords = {Business \& Economics / Business Mathematics, Business \& Economics / Industries / Computers \& Information Technology, Business \& Economics / Skills, Computers / Computer Graphics, Computers / Data Modeling \& Design, Computers / Data Processing, Computers / Data Visualization, Computers / Desktop Applications / Design \& Graphics},
}

@misc{modrak_simulation-based_2022,
	title = {Simulation-{Based} {Calibration} {Checking} for {Bayesian} {Computation}: {The} {Choice} of {Test} {Quantities} {Shapes} {Sensitivity}},
	shorttitle = {Simulation-{Based} {Calibration} {Checking} for {Bayesian} {Computation}},
	url = {http://arxiv.org/abs/2211.02383},
	abstract = {Simulation-based calibration checking (SBC) is a practical method to validate computationally-derived posterior distributions or their approximations. In this paper, we introduce a new variant of SBC to alleviate several known problems. Our variant allows the user to in principle detect any possible issue with the posterior, while previously reported implementations could never detect large classes of problems including when the posterior is equal to the prior. This is made possible by including additional data-dependent test quantities when running SBC. We argue and demonstrate that the joint likelihood of the data is an especially useful test quantity. Some other types of test quantities and their theoretical and practical benefits are also investigated. We support our recommendations with numerical case studies on a multivariate normal example and theoretical analysis of SBC, thereby providing a more complete understanding of the underlying statistical mechanisms. From the theoretical side, we also bring attention to a relatively common mistake in the literature and clarify the difference between SBC and checks based on the data-averaged posterior. The SBC variant introduced in this paper is implemented in the SBC R package.},
	urldate = {2022-12-21},
	publisher = {arXiv},
	author = {Modrák, Martin and Moon, Angie H. and Kim, Shinyoung and Bürkner, Paul and Huurre, Niko and Faltejsková, Kateřina and Gelman, Andrew and Vehtari, Aki},
	month = nov,
	year = {2022},
	note = {arXiv:2211.02383 [stat]},
	keywords = {Statistics - Methodology},
}

@misc{sailynoja_graphical_2021,
	title = {Graphical {Test} for {Discrete} {Uniformity} and its {Applications} in {Goodness} of {Fit} {Evaluation} and {Multiple} {Sample} {Comparison}},
	url = {http://arxiv.org/abs/2103.10522},
	abstract = {Assessing goodness of fit to a given distribution plays an important role in computational statistics. The Probability integral transformation (PIT) can be used to convert the question of whether a given sample originates from a reference distribution into a problem of testing for uniformity. We present new simulation and optimization based methods to obtain simultaneous confidence bands for the whole empirical cumulative distribution function (ECDF) of the PIT values under the assumption of uniformity. Simultaneous confidence bands correspond to such confidence intervals at each point that jointly satisfy a desired coverage. These methods can also be applied in cases where the reference distribution is represented only by a finite sample. The confidence bands provide an intuitive ECDF-based graphical test for uniformity, which also provides useful information on the quality of the discrepancy. We further extend the simulation and optimization methods to determine simultaneous confidence bands for testing whether multiple samples come from the same underlying distribution. This multiple sample comparison test is especially useful in Markov chain Monte Carlo convergence diagnostics. We provide numerical experiments to assess the properties of the tests using both simulated and real world data and give recommendations on their practical application in computational statistics workflows.},
	urldate = {2022-12-21},
	publisher = {arXiv},
	author = {Säilynoja, Teemu and Bürkner, Paul-Christian and Vehtari, Aki},
	month = nov,
	year = {2021},
	note = {arXiv:2103.10522 [stat]},
	keywords = {Statistics - Methodology},
}

@misc{gabry_plotting_2022,
	type = {R package version 1.10.0},
	title = {Plotting for {Bayesian} {Models}},
	url = {https://mc-stan.org/bayesplot/},
	abstract = {bayesplot is an R package providing an extensive library of plotting functions for use after fitting Bayesian models (typically with MCMC). The plots created by bayesplot are ggplot objects, which means that after a plot is created it can be further customized using various functions from the ggplot2 package.

Currently bayesplot offers a variety of plots of posterior draws, visual MCMC diagnostics, and graphical posterior (or prior) predictive checking. Additional functionality (e.g. for forecasting/out-of-sample prediction and other inference-related tasks) will be added in future releases.

The idea behind bayesplot is not only to provide convenient functionality for users, but also a common set of functions that can be easily used by developers working on a variety of packages for Bayesian modeling, particularly (but not necessarily) those powered by RStan.},
	language = {en},
	urldate = {2023-04-12},
	journal = {Bayesplot},
	author = {Gabry, Jonah and Mahr, Tristan},
	year = {2022},
}

@misc{kay_ggdist_2023,
	title = {ggdist: {Visualizations} of distributions and uncertainty},
	copyright = {Open Access},
	shorttitle = {ggdist},
	url = {https://zenodo.org/record/3879620},
	abstract = {New features and enhancements: Support for non-numeric distributions in {\textless}code{\textgreater}stat\_slabinterval(){\textless}/code{\textgreater} and {\textless}code{\textgreater}stat\_dotsinterval(){\textless}/code{\textgreater}, including {\textless}code{\textgreater}dist\_categorical(){\textless}/code{\textgreater}, {\textless}code{\textgreater}dist\_bernoulli(){\textless}/code{\textgreater}, and the upcoming {\textless}code{\textgreater}posterior::rvar\_factor(){\textless}/code{\textgreater} type. (\#108) Various improvements to dotplot layout in {\textless}code{\textgreater}geom\_dotsinterval(){\textless}/code{\textgreater}: new {\textless}code{\textgreater}layout = "hex"{\textless}/code{\textgreater} allows a hexagonal circle-packing style layout (\#161). new mechanism for smoothing dotplots using the {\textless}code{\textgreater}smooth{\textless}/code{\textgreater} parameter, including {\textless}code{\textgreater}smooth = "bounded"{\textless}/code{\textgreater} / {\textless}code{\textgreater}smooth = "unbounded"{\textless}/code{\textgreater} (for "density dotplots") and {\textless}code{\textgreater}smooth = "discrete"{\textless}/code{\textgreater} / {\textless}code{\textgreater}smooth = "bar"{\textless}/code{\textgreater} (for improved layout of large-n discrete distributions). (\#161) a better bin/dot-nudging algorithm using constrained optimization (\#163) new {\textless}code{\textgreater}overlaps = "keep"{\textless}/code{\textgreater} option disables bin/dot nudging in {\textless}code{\textgreater}"bin"{\textless}/code{\textgreater}, {\textless}code{\textgreater}"hex"{\textless}/code{\textgreater}, and {\textless}code{\textgreater}"weave"{\textless}/code{\textgreater} layouts. This means {\textless}code{\textgreater}layout = "weave"{\textless}/code{\textgreater} with {\textless}code{\textgreater}overlaps = "keep"{\textless}/code{\textgreater} will yield exact dot positions. (\#161) The {\textless}code{\textgreater}"weave"{\textless}/code{\textgreater} layout now works properly with {\textless}code{\textgreater}side = "both"{\textless}/code{\textgreater} fixed binning artifacts when there is high density on the edges, particularly right edges (\#144) use a max {\textless}code{\textgreater}binwidth{\textless}/code{\textgreater} of 1 for discrete distributions (\#159) new {\textless}code{\textgreater}overflow = "compress"{\textless}/code{\textgreater} allows layouts to be compressed to fit into the geom bounds if a user-specified {\textless}code{\textgreater}binwidth{\textless}/code{\textgreater} would otherwise cause the dots to exceed the geom bounds. (\#162) Two new shortcut geoms for {\textless}code{\textgreater}geom\_dotsinterval(){\textless}/code{\textgreater}: {\textless}code{\textgreater}geom\_swarm(){\textless}/code{\textgreater} and {\textless}code{\textgreater}geom\_weave(){\textless}/code{\textgreater}. Both can be used to quickly create "beeswarm"-like plots. A new "mirrored" scale for the {\textless}code{\textgreater}side{\textless}/code{\textgreater} aesthetic, {\textless}code{\textgreater}scale\_side\_mirrored(){\textless}/code{\textgreater}, makes it easier to create mirrored slabs and dotplots. (\#142) Custom density estimators can now be used with {\textless}code{\textgreater}stat\_slabinterval(){\textless}/code{\textgreater} via the {\textless}code{\textgreater}density{\textless}/code{\textgreater} argument, including a new bounded density estimator ({\textless}code{\textgreater}density\_bounded(){\textless}/code{\textgreater}). (\#113) Following the split between {\textless}code{\textgreater}size{\textless}/code{\textgreater} and {\textless}code{\textgreater}linewidth{\textless}/code{\textgreater} aesthetics in {\textless}em{\textgreater}ggplot2{\textless}/em{\textgreater} 3.4, the following aesthetics have been updated (\#138): {\textless}code{\textgreater}interval\_size{\textless}/code{\textgreater} is now {\textless}code{\textgreater}linewidth{\textless}/code{\textgreater} {\textless}code{\textgreater}slab\_size{\textless}/code{\textgreater} is now {\textless}code{\textgreater}slab\_linewidth{\textless}/code{\textgreater} in {\textless}code{\textgreater}geom\_slab(){\textless}/code{\textgreater}, {\textless}code{\textgreater}geom\_dots(){\textless}/code{\textgreater}, and {\textless}code{\textgreater}geom\_lineribbon(){\textless}/code{\textgreater}, {\textless}code{\textgreater}size{\textless}/code{\textgreater} is now {\textless}code{\textgreater}linewidth{\textless}/code{\textgreater} A new {\textless}strong{\textgreater}experimental{\textless}/strong{\textgreater} mini domain-specific language for probability expressions in {\textless}em{\textgreater}ggdist{\textless}/em{\textgreater} {\textless}code{\textgreater}stat{\textless}/code{\textgreater}s: the {\textless}code{\textgreater}Pr\_(){\textless}/code{\textgreater} and {\textless}code{\textgreater}p\_(){\textless}/code{\textgreater} functions can be used to generate {\textless}code{\textgreater}after\_stat(){\textless}/code{\textgreater} expressions in terms of {\textless}em{\textgreater}ggdist{\textless}/em{\textgreater} computed variables; e.g. {\textless}code{\textgreater}aes(thickness = !!Pr\_(X \&lt;= x)){\textless}/code{\textgreater} maps the CDF of the distribution onto the {\textless}code{\textgreater}thickness{\textless}/code{\textgreater} aesthetic; {\textless}code{\textgreater}aes(thickness = !!p\_(x)){\textless}/code{\textgreater} maps the PDF onto the {\textless}code{\textgreater}thickness{\textless}/code{\textgreater} aesthetic. (\#160) Several function families in {\textless}em{\textgreater}ggdist{\textless}/em{\textgreater} now use "currying" (automatic partial function application). These function families partially apply themselves until all non-optional arguments have been supplied: {\textless}code{\textgreater}point\_interval(){\textless}/code{\textgreater}, {\textless}code{\textgreater}smooth\_...{\textless}/code{\textgreater}, and {\textless}code{\textgreater}density\_...{\textless}/code{\textgreater}. See {\textless}code{\textgreater}help("automatic-partial-functions"){\textless}/code{\textgreater}. Performance improvements for {\textless}code{\textgreater}point\_interval(){\textless}/code{\textgreater} on grouped data frames. (\#154) Documentation: Uses of {\textless}code{\textgreater}stat(){\textless}/code{\textgreater} have been replaced with {\textless}code{\textgreater}after\_stat(){\textless}/code{\textgreater} to be consistent with the deprecation of {\textless}code{\textgreater}stat(){\textless}/code{\textgreater} in {\textless}em{\textgreater}ggplot2{\textless}/em{\textgreater} 3.4.},
	urldate = {2023-04-12},
	publisher = {Zenodo},
	author = {Kay, Matthew},
	month = jan,
	year = {2023},
	doi = {10.5281/ZENODO.3879620},
}

@book{gelman_regression_2020,
	address = {S.l.},
	title = {Regression and {Other} {Stories}},
	isbn = {978-1-107-02398-7},
	language = {English},
	publisher = {Cambridge University Press},
	author = {Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
	year = {2020},
	note = {OCLC: 1150969622},
}

@article{kleiber_visualizing_2016,
	title = {Visualizing {Count} {Data} {Regressions} {Using} {Rootograms}},
	volume = {70},
	issn = {0003-1305},
	url = {https://doi.org/10.1080/00031305.2016.1173590},
	doi = {10.1080/00031305.2016.1173590},
	abstract = {The rootogram is a graphical tool associated with the work of J. W. Tukey that was originally used for assessing goodness of fit of univariate distributions. Here, we extend the rootogram to regression models and show that this is particularly useful for diagnosing and treating issues such as overdispersion and/or excess zeros in count data models. We also introduce a weighted version of the rootogram that can be applied out of sample or to (weighted) subsets of the data, for example, in finite mixture models. An empirical illustration revisiting a well-known dataset from ethology is included, for which a negative binomial hurdle model is employed. Supplementary materials providing two further illustrations are available online: the first, using data from public health, employs a two-component finite mixture of negative binomial models; the second, using data from finance, involves underdispersion. An R implementation of our tools is available in the R package countreg. It also contains the data and replication code.},
	number = {3},
	urldate = {2023-04-12},
	journal = {The American Statistician},
	author = {Kleiber, Christian and Zeileis, Achim},
	month = jul,
	year = {2016},
	keywords = {Finite mixture, Goodness of fit, Hurdle model, Negative binomial regression, Poisson regression},
	pages = {296--303},
}

@article{dimitriadis_stable_2021,
	title = {Stable reliability diagrams for probabilistic classifiers},
	volume = {118},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.2016191118},
	doi = {10.1073/pnas.2016191118},
	abstract = {Significance
            Probabilistic classifiers assign predictive probabilities to binary events, such as rainfall tomorrow, a recession, or a personal health outcome. Such a system is reliable or calibrated if the predictive probabilities are matched by the observed frequencies. In practice, calibration is assessed graphically in reliability diagrams and quantified via the reliability component of mean scores. Extant approaches rely on binning and counting and have been hampered by ad hoc implementation decisions, a lack of reproducibility, and inefficiency. Here, we introduce the CORP approach, which uses the pool-adjacent-violators algorithm to generate optimally binned, reproducible, and provably statistically consistent reliability diagrams, along with a numerical measure of miscalibration based on a revisited score decomposition.
          ,
            A probability forecast or probabilistic classifier is reliable or calibrated if the predicted probabilities are matched by ex post observed frequencies, as examined visually in reliability diagrams. The classical binning and counting approach to plotting reliability diagrams has been hampered by a lack of stability under unavoidable, ad hoc implementation decisions. Here, we introduce the CORP approach, which generates provably statistically consistent, optimally binned, and reproducible reliability diagrams in an automated way. CORP is based on nonparametric isotonic regression and implemented via the pool-adjacent-violators (PAV) algorithm—essentially, the CORP reliability diagram shows the graph of the PAV-(re)calibrated forecast probabilities. The CORP approach allows for uncertainty quantification via either resampling techniques or asymptotic theory, furnishes a numerical measure of miscalibration, and provides a CORP-based Brier-score decomposition that generalizes to any proper scoring rule. We anticipate that judicious uses of the PAV algorithm yield improved tools for diagnostics and inference for a very wide range of statistical and machine learning methods.},
	language = {en},
	number = {8},
	urldate = {2023-04-12},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Dimitriadis, Timo and Gneiting, Tilmann and Jordan, Alexander I.},
	month = feb,
	year = {2021},
	pages = {e2016191118},
}

@article{gabry_visualization_2019,
	title = {Visualization in {Bayesian} workflow},
	volume = {182},
	issn = {0964-1998, 1467-985X},
	url = {http://arxiv.org/abs/1709.01449},
	doi = {10.1111/rssa.12378},
	abstract = {Bayesian data analysis is about more than just computing a posterior distribution, and Bayesian visualization is about more than trace plots of Markov chains. Practical Bayesian data analysis, like all data analysis, is an iterative process of model building, inference, model checking and evaluation, and model expansion. Visualization is helpful in each of these stages of the Bayesian workflow and it is indispensable when drawing inferences from the types of modern, high-dimensional models that are used by applied researchers.},
	number = {2},
	urldate = {2021-11-17},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew},
	month = feb,
	year = {2019},
	note = {tex.ids= gabryVisualizationBayesianWorkflow2019a
arXiv: 1709.01449},
	keywords = {Statistics - Applications, Statistics - Methodology},
	pages = {389--402},
}

@article{gelman_bayesian_2020,
	title = {Bayesian {Workflow}},
	url = {http://arxiv.org/abs/2011.01808},
	abstract = {The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.},
	urldate = {2020-11-04},
	journal = {arXiv:2011.01808 [stat]},
	author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C. and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and Bürkner, Paul-Christian and Modrák, Martin},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.01808},
	keywords = {Statistics - Methodology},
}

@article{gelman_bayesian_nodate,
	title = {Bayesian {Data} {Analysis}},
	language = {en},
	author = {Gelman, Andrew and Carlin, John B and Stern, Hal S and Dunson, David B and Vehtari, Aki and Rubin, Donald B},
	pages = {677},
}

@inproceedings{kay_when_2016,
	address = {San Jose California USA},
	title = {When (ish) is {My} {Bus}?: {User}-centered {Visualizations} of {Uncertainty} in {Everyday}, {Mobile} {Predictive} {Systems}},
	isbn = {978-1-4503-3362-7},
	shorttitle = {When (ish) is {My} {Bus}?},
	url = {https://dl.acm.org/doi/10.1145/2858036.2858558},
	doi = {10.1145/2858036.2858558},
	abstract = {Users often rely on realtime predictions in everyday contexts like riding the bus, but may not grasp that such predictions are subject to uncertainty. Existing uncertainty visualizations may not align with user needs or how they naturally reason about probability. We present a novel mobile interface design and visualization of uncertainty for transit predictions on mobile phones based on discrete outcomes. To develop it, we identified domain specific design requirements for visualizing uncertainty in transit prediction through: 1) a literature review, 2) a large survey of users of a popular realtime transit application, and 3) an iterative design process. We present several candidate visualizations of uncertainty for realtime transit predictions in a mobile context, and we propose a novel discrete representation of continuous outcomes designed for small screens, quantile dotplots. In a controlled experiment we find that quantile dotplots reduce the variance of probabilistic estimates by {\textasciitilde}1.15 times compared to density plots and facilitate more confident estimation by end-users in the context of realtime transit prediction scenarios.},
	language = {en},
	urldate = {2022-09-13},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Kay, Matthew and Kola, Tara and Hullman, Jessica R. and Munson, Sean A.},
	month = may,
	year = {2016},
	pages = {5092--5103},
}

@article{wilkinson_dot_1999,
	title = {Dot {Plots}},
	volume = {53},
	issn = {00031305},
	url = {https://www.jstor.org/stable/2686111?origin=crossref},
	doi = {10.2307/2686111},
	number = {3},
	urldate = {2023-04-12},
	journal = {The American Statistician},
	author = {Wilkinson, Leland},
	month = aug,
	year = {1999},
	pages = {276},
}

@book{gelman2013bayesian,
  title={Bayesian Data Analysis},
  author={Gelman, Andrew and Carlin, John B and Stern, Hal S and Dunson, David B and Vehtari, Aki and Rubin, Donald B},
  year={2013},
  publisher={Chapman and Hall/CRC}
}
